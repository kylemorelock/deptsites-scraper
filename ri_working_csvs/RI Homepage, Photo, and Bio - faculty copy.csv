http://www.ri.cmu.edu/ri_people.html?fcode=1&menu_id=251&distype=detail&dis_type=detail&list_type=current&selected_letter=AZ,http://www.ri.cmu.edu,,,,http://www.ri.cmu.edu/,,,,
,,,,,,,,,
,NAME,EMAIL,TITLE,DEPARTMENT,PHOTO,HOMEPAGE,BIO,department_website,terminal command
,,,,,,,,,
QUERY,//font[@style='font-size:15px;font-weight:bold;'],,//tr[2]/td/b,,"//tr/td/a/img/@src[contains(.,'people')]",//td[@height='10']/a/@href,//td[@class='td_text']/p,//font/a[@href],
,,,,,,,,,
RESPONSE,Dimitrios (Dimi) Apostolopoulos,da1v@andrew.cmu.edu,"Senior Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/apostolopoulos_dimi_2.jpg,http://www.ri.cmu.edu/people/apostolopoulos%20dimitrios.html,"I am a Senior Systems Scientist at the Robotics Institute of Carnegie Mellon University and the National Robotics Engineering Center (NREC). My work focuses on using robotics for discovery and exploration, and as a workforce for hazardous duty and in demanding applications.</p>
<p class='cmsP'>Since 1998, I have led recognized robotic programs and produced robotics systems and technology  for Robotic Search for Antarctic Meteorites (1998-2000), Unmanned Ground Combat Vehicle (2001-2002),Gladiator (2002-2004), Tactical Unmanned Ground Vehicle (2005-2008), and Safe Operations of Unmanned systems for Reconnaissance in Complex Environments (2010-2012).</p>
<p class='cmsP'>Currently I lead research and development of autonomous robots for mining of noble metals, novel robotic mechanisms for all-terrain mobility, and new methods for design and prototyping of unmanned ground vehicles.",http://www.ri.cmu.edu/person.html?person_id=3,open http://www.ri.cmu.edu/person.html?person_id=3
,Chris Atkeson,cga@andrew.cmu.edu,"Professor, RI/HCII",Robotics Institute,http://www.ri.cmu.edu/images/people/atkeson_christopher.jpg,#VALUE!,#VALUE!,,open 
,J. Andrew (Drew) Bagnell,bagnell2@andrew.cmu.edu,"Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/bagnell_drew_05.JPG,http://www.robotwhisperer.org,"I am interested in ""closing the loop"" on complex systems; that is, I am interested in designing algorithms that allow systems to observe their own operation and improve performance. My belief is that the border land between planning, control and computational learning is particularly rich with research challenges and potential to make real, immediate impact on applications. I'm particularly interested in systems for which we can obtain at best a partial model. To this end, I'm excited about extending research tools that come from information theory, statistics, control theory, statistical physics and optimization.At the moment, I am particularly focused on two areas in machine learning. First I am working on applications of learning and decision making applied to mobile robotics. Second, I am interested in developing rich, structured probabilistic models that are appropriate for both making and learning decisions.",http://www.ri.cmu.edu/person.html?person_id=689,open http://www.ri.cmu.edu/person.html?person_id=689
,David Bourne,bourne@andrew.cmu.edu,"Principal Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/bourne.png,#N/A,"My research is focused on building intelligent systems for automated manufacturing. The ultimate goal is to design an artifact and send the design to an automated production facility that would produce it. I am currently investigating several research issues in the design area:After a design is complete, it is passed to a production facility. Here, a setup and process plan is prepared along with descriptions of machine setups. The setup descriptions and the stock material are sent to the manufacturing workstations that automatically assemble the part into the fixtures. The manufacturing step is completed for that setup and for all subsequent setups, until the final part is correct and complete.And finally, we are building the hardware environment that will sufficiently structure the environment to permit the automated production of one-off production quality parts.",http://www.ri.cmu.edu/person.html?person_id=26,open http://www.ri.cmu.edu/person.html?person_id=26
,Howie Choset,choset@andrew.cmu.edu,"Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/choset_howie_2013.jpg,http://www.cs.cmu.edu/~choset,"My goal has been, and remains, to bring the precision of computer science and applied mathematics to the realities and uncertainties of mechanical systems. I will continue to make fundamental contributions in design, motion planning, path planning, and estimation.Mechanism Design. My group has designed and constructed a variety of snake robots, highly articulated mechanisms that can exploit their shape and many degrees of freedom to thread through tightly packed regions, accessing locations that people and conventional machinery otherwise could not. Working with practitioners in their respective fields, my group has been active in applying these mechanisms toward minimally invasive surgery, urban search and rescue, and the manufacturing of aircraft wing assemblies with Boeing. One notable example is our surgical snake robot; in collaboration with Dr. Marco Zenati at the University of Pittsburgh, we have developed a small 11mm cross-sectional diameter snake robot for use in minimally invasive surgery. This device can reach deep into the body without the need for large incisions, thereby reducing both post-operative patient discomfort and medical costs. I am now working with Dr. Shyam Thakkar at Allegheny General Hospital and Dr. Lee Swanstrom at the Oregon Clinic to develop technology to conduct natural orifice transluminal endoscopic surgery (NOTES). NOTES uses the natural orifices to provide access to the abdominal cavity through the walls of a luminal space in order to perform surgical procedures, thereby avoiding invasive abdominal wall incisions. Even more than conventional minimally invasive surgery, NOTES has the potential to produce better outcomes and quicker recovery as well as less post-operative discomfort and fewer complications.We are also active in developing locomoting snake robots that use their internal degrees of freedom to interact with the environment in order to propel themselves in any desired direction. Our locomoting devices, based on Mark Yim’s earlier work, have a modular design where each module comprises its own actuator, onboard sensing and custom electronics, all contained within a novel mechatronic architecture. In the last two years alone, we have deployed this robot in many locations: Southwest Research Institute, Texas Task Force I’s Disaster City, power plants, and the House of Representatives’ Rayburn Building. Finally, included in Figure 2b-e, are other mechanisms my group has worked on over the past 10 years.Motion Planning. Simple algorithms alone are not sufficient to coordinate all of the degrees of freedom needed to produce purposeful motion for a complex robot. Motion planning for snake robots is difficult because they have many degrees of freedom and they are also underactuated, meaning that we can only control the internal degrees of freedom of the robot; the net motion cannot be directly controlled. Therefore, we designed motion planners to coordinate the internal degrees of freedom so that the induced shape changes propel the snake robot in a desired direction. Already, we have been able to derive and implement a wide variety of snake robot gaits, including climbing, gap crossing, swimming, and stair climbing. These gaits have been demonstrated in field tests, both inside and outside. We are currently looking at the fundamental mechanics to derive optimal gaits for a broad class of dynamic systems with nonholonomic constraints. Finally, Prof. Jeff Schneider and I have started a collaboration to develop multi-objective learning techniques that search the parameter space of our gait model for a set of “equally” optimal (pareto optimal) parameters, rather than optimizing a single weighted sum of the objectives. Through collaboration with Prof. Matt Mason and our co-advised students we have developed a single degree-of-freedom robot which uses dynamic motion to climb between two parallel vertical walls. This work introduces minimalism in climbing, using dynamic motions to enable open loop control with a single motor. We derive the nonlinear, piecewise smooth dynamical equations of motion in an eight dimensional state space, derive its Poincaré map, and use it to reduce the dynamics to three dimensions.Path planning. Whereas motion planners specify a sequence of configurations, velocities and perhaps higher order derivatives to produce net motion, a path planner determines a sequence of robot configurations between a start and goal. Although the two tasks are distinct, they both contend with the complexity of high dimensional configuration spaces. To deal with this complexity for path planning, our approach uses a topological map of the space, which reduces planning from a multi-dimensional search problem to a one-dimensional search.Our work in topological mapping led to developing a coverage path planner that directs a robot to pass over every point in a target region, for application in robotic demining. This work represents the first provably complete sensor-based coverage algorithm for a broad class of patterns. Dr. Rizzi and I then applied this coverage technology to the application of autobody painting with the Ford Motor Company to expedite the painting operation while minimizing hazardous waste. Spray deposition is not just a coverage problem in three dimensions but a uniform coverage one; we must regulate the path and speed of the applicator to account for the paint deposition process and the impact of surface curvature to ensure uniform coverage.Estimation. One of the issues in exploring unknown spaces is localization while mapping, often referred to as simultaneous localization and mapping (SLAM). One of the critical challenges of SLAM deals with the computational complexity of mapping large spaces. We developed a hierarchical SLAM technique to address this problem that uses a topological map to divide the space into smaller regions, each represented by a high resolution map. Therefore, instead of trying to localize in a large map, our hybrid technique instead uses the topological map to select on which local map to localization. In collaboration with Dr. George Kantor, we recently derived a multi-hypothesis, probabilistically grounded method for detecting loop closure when exploring unknown environments [23, 24]. This work further improves the computational efficiency and success rate of our topological SLAM algorithm.Finally, undulatory motion planners require a technique to estimate the robot’s state so as to more effectively control the robot’s gait. In collaboration with Dr. Rizzi, we developed techniques to estimate the state of a highly dynamic robot, RHex. This technique employs a multi-modal estimator to selectively disable and enable the appropriate filters at the appropriate time. This leads to improved scalability, because fewer filters are running simultaneously, as well as improved accuracy, because “averaging” is not corrupted by incorrect filter outputs [26, 27].Hybrid Controls. Recall that path planning directs the robot where to go, whereas motion planning determines how to go. In other words, path planning typically assumes full control over all the configuration variables and respects obstacles, while underactuated motion planning assumes no obstacles and only has control over a subset of the configuration variables. Ultimately, we seek to combine our path planning and motion planning work by using our estimation techniques to produce feedback for the system. As a first step toward this goal, Dr. Rizzi and I have combined his prior control work with my path planning efforts to form a new hybrid controls framework that combines discrete and continuous planning where an arbitrator selects which continuous feedback control law to invoke. Specifically, we define a series of controllers that are valid over simple domains such that one controller in the sequence brings the robot into the domain of the next controller, and therefore the sequence collectively brings the robot to a goal state. In a sense, instead of using path planning to determine a “thin” path, we now have a procedure that determines a “fat” path or vector field along which the robot “flows” from a start location to a goal. This extends sequential composition in two ways: i) our controllers flow into the next policy, as opposed to converging to a single state in the next domain, and ii) our controllers respect non-holonomic constraints.It is the excitement of working with students that continues to draw me to academia. I am certain that a casual tour of my lab reveals a feeling of energy and productivity. My students, both graduate and undergraduate, work hard to provide fresh new insights within the framework of mathematical and experimental rigor endowed by my research program. This philosophy of theoretic and applied rigor has been the cornerstone of my graduate course, Robotic Motion Planning (16-735), which has resulted in a co-authored textbook on motion planning. Also, guided by this philosophy of mathematical and experimental rigor, I designed and continue to teach an undergraduate class, Introduction to Robotics (16-311), that makes use of LEGO robots to reinforce basic principles taught in class. This course provides the basis for the Undergraduate Robotics Minor, which I founded in 1998 and have directed since its inception. This minor is one of the first undergraduate certifications for robotics in the world. Building off the success of the minor, I introduced a fifth year Robotics Masters Program for Carnegie Mellon undergraduates.",http://www.ri.cmu.edu/person.html?person_id=47,open http://www.ri.cmu.edu/person.html?person_id=47
,Stelian Coros,scoros@andrew.cmu.edu,"Assistant Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/coros_crop.jpg,#N/A,"My research lies at the intersection of Computer Graphics, Robotics, Biomechanics and Fabrication. More specifically, I am interested in developing models of motor control, motion planning algorithms, physics-based simulation tools and computational design methods for 3d printable mechanical structures. The long term goal of my research is to make the process of designing lifelike robots as intuitive and accessible as creating virtual characters is today.",http://www.ri.cmu.edu/person.html?person_id=3309,open http://www.ri.cmu.edu/person.html?person_id=3309
,Keenan Crane,keenanc@andrew.cmu.edu,"Assistant Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/Keenan-Crane.jpg,http://www.cs.columbia.edu/~keenan/,"Keenan's research draws on insights from differential geometry and computer science to develop fundamental algorithms for working with real-world geometric data. This work addresses a growing need for geometric understanding not only in traditional domains like science and engineering, but also in rapidly developing consumer technologies like 3D fabrication and augmented reality.",http://www.ri.cmu.edu/person.html?person_id=3431,open http://www.ri.cmu.edu/person.html?person_id=3431
,Fernando De la Torre Frade,ftorre@andrew.cmu,"Associate Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/delatorrefrade_fernando.jpg,http://www.cs.cmu.edu/~ftorre,"Dr. De la Torre's research interests include machine learning, signal processing and computer vision, with a focus on understanding human behavior from multimodal sensors (e.g. video, body sensors). I am particularly interested in three main topics:",http://www.ri.cmu.edu/person.html?person_id=1067,open http://www.ri.cmu.edu/person.html?person_id=1067
,John M. Dolan,jdolan@andrew.cmu.edu,"Principal Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/Dolan_John_2013.jpg,http://www.ri.cmu.edu/person.html?person_id=69,"My research goal is to create systems and methodologies that allow groups of robots and humans to collaborate with one another to perform useful tasks. My research is motivated by an interest in the modeling and control of physical systems on the one hand, and in the creation of effective human-machine interfaces on the other. Given continuing technological advances in computing, sensing, actuation, and miniaturization, I believe that the nexus of these two interests is an increasingly exciting area to explore.The pursuit of this research involves two complementary thrusts. On the one hand, robot systems should be endowed with maximal autonomy. Perception, actuation, planning, and even tasking should be performed with the smallest amount of human intervention possible. Human attention and intelligence are then freed for supervisory and remedial actions that transcend the current competence of the robot or group of robots being controlled. On the other hand, human users should have increasingly effective means of tasking, controlling, and communicating with robots. Human control of robots then becomes less burdensome and more intuitive, and users are able to oversee more complex tasks with greater numbers of robots.Autonomous Driving. We are developing algorithms for urban and highway driving that enable more anthropomorphic autonomous behaviors than previously possible, and we are seeking to combine these with safety guarantees for a result that combines human-like fluidity with the higher safety potential of rapid-response computing and actuation. We are also investigating shared human-vehicle autonomy strategies.Telesupervisory human-robot systems. I want to build systems that allow maximally intuitive human input in controlling large numbers of machines with variable autonomy. The focus of my earlier NASA project “Wide-Area Prospecting Using Supervised Autonomous Robots” was to create a telesupervision system allowing a single astronaut in a “shirtsleeve” environment to control multiple rovers performing a prospecting task, increasing astronaut safety and productivity. The resultant telesupervision architecture is applicable to a wide variety of tasks, to include exploration, space assembly, inspection, and maintenance. Subsequent projects have applied the telesupervision architecture to multiple robot surface craft detecting Harmful Algal Blooms (HAB) and performing water quality assessment.Robot reliability. Mobile robots are typically unreliable. NASA has expressed interest in using modular self-repairable robotic teams for the exploration and colonization of Mars. The use of modular, self-repairing robot teams adds new complexity to the mission design process for robotic exploration. Decisions must be made about how to divide tasks among multiple robots and how to configure the robots and teams to accomplish both individual tasks and overall mission goals. We are using systems engineering reliability principles to answer questions like: ""What is the lowest-cost configuration of robots that will accomplish a given set of mission tasks with a given probability of success?"" This analysis allows comparison of teams of repairable vs. non-repairable robots, teams where the robots use components with different reliabilities, and teams with different numbers of robots and different numbers of spare parts.",http://www.ri.cmu.edu/person.html?person_id=69,open http://www.ri.cmu.edu/person.html?person_id=69
,Artur W. Dubrawski,awd@andrew.cmu.edu,"Senior Systems Scientist/Adjunct Professor MISM, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/dubrawski_artur3.jpg,http://www.autonlab.org/autonweb/10223.html,"I am interested in intelligent systems that work, are useful, and make economic sense, and in finding ways to effectively build and deploy them. My work is driven by real-world applications, currently in the areas of public health, food safety, nuclear safety and health of equipment. It involves researching new machine learning algorithms and data structures to facilitate probabilistic modeling, predictive analysis, interactive exploration, and understanding of data.",http://www.ri.cmu.edu/person.html?person_id=1272,open http://www.ri.cmu.edu/person.html?person_id=1272
,Michael Erdmann,me51@andrew.cmu.edu,"Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/erdmann_michael.jpg,http://www.cs.cmu.edu/~me/,"I am interested in making robots act purposefully and successfully in a world in which most everything is uncertain. Sensors are noisy, actions are imprecise, and objects are often in the wrong location. Despite such obstacles to purposeful action, there are many tasks that can be accomplished successfully. Humans, animals, and some machines are proof. Providing robots with the ability to operate autonomously and purposefully requires an understanding of how different tasks may be accomplished by different repertoires of actions. Grasping, hitting, and dropping are some actions that are useful in a robot's repertoire. More exotic actions include shaking, twirling, and other actions that randomize an object's state. Recently I constructed a ""two-palm robot"". The robot consisted of two manipulator arms cooperating to manipulate objects without the need for full kinematic constraint. The arms ""programmed themselves"", that is, they invoked an automatic planner to find sequences of motions for reorienting objects in their palms. The planner built a geometric graph based on a critical event analysis of the underlying mechanics.My work is motivated by several desires. First, I would like to program robots more easily than is currently possible. Second, I would like to understand the scope and limitations of autonomous systems, whether biological or artificial. Third, I would like to reduce the complexity of design and planning by codifying the design parameters required to achieve a given level of automation. An underlying goal of my research is to understand the relationship between sensing, action, and prediction. In the past, I have explored various extreme points in this space. With Matt Mason I explored sensorless strategies, for my thesis work I looked at randomized strategies, and most recently I investigated fast-action minimal-sensing strategies. My research draws on tools from geometry, mechanics, planning, and stochastic processes.I am interested in sensing strategies that acquire object shape and configuration concurrently during manipulation, a research direction pioneered by my former students Yan-Bin Jia and Mark Moll. Currently, Matt Mason, Sidd Srinivasa, and I are working on a related project to develop a theory of task-level kinesthetic perception.I am also interested in protein homology, in particular determining structural homology from sparse NMR data. I am exploring techniques from knot theory to model protein structures.",http://www.ri.cmu.edu/person.html?person_id=74,open http://www.ri.cmu.edu/person.html?person_id=74
,Gary K. Fedder,fedder@andrew.cmu.edu,"Vice Provost for Research, RI/ECE",Robotics Institute,http://www.ri.cmu.edu/images/people/fedder_gary2.jpg,#N/A,"As information systems have evolved from isolated computational engines to distributed networks, the autonomous ability to gather and act on information is becoming increasingly important. My research is in the interdisciplinary area of MicroElectroMechanical Systems (MEMS): sensor and actuator systems with performance derived from integration of electronics and mechanical structures with features measured in microns to millimeters. Fabrication of the batch-fabricated electromechanical devices and the development of related processes leverage the enormous investment in mature Very-Large-Scale Integrated (VLSI) circuit manufacturing. Benefits of this approach include much lower manufacturing cost, greater miniaturization, greater integration, and in many cases higher performance than can be achieved with conventional methods used to build systems requiring sensors and actuators. My research focus on integrated MEMS will eventually lead to the manufacture of low-cost sensor-and-actuator Application-Specific Integrated Circuits (ASICs). Integrated MEMS technology will be pervasive in future embedded systems.Our research group designs, fabricates, and tests microdevices that are primarily made using a process in high conventional foundry CMOS is followed by simple micromachining steps. This process provides us with high-performance electronics integrated on chip with electrostatically actuated microstructures, capacitive and piezoresistive sensors, and polysilicon thermal heaters. Projects include micromechanisms for magnetic probe-based data storage, accelerometers and gyroscopes for inertial sensing, and ciliary sensors for tactile and acoustic imaging. Of particular interest is how large arrays of these sensors and actuators may improve overall system-level performance. Issues include system design and integration, distributed control and communication, and interfacing to the environment.MEMS are coupled multi-domain systems and, therefore, are difficult to design without expertise in a diverse set of fields. To address this problem in our lab, MEMS designers and CAD developers work closely together in a synergetic research environment. We are developing a multi-domain hierarchical design methodology to speed up the design cycle. A MEMS schematic is being developed in which mechanical, electromechanical, and electronic elements are graphically interconnected, resulting in rapid simulation and evaluation of designs. We are also modeling topologies for common MEMS applications, such as accelerometry, to codify design constraints for use in automated synthesis tools.",http://www.ri.cmu.edu/person.html?person_id=79,open http://www.ri.cmu.edu/person.html?person_id=79
,John Galeotti,jgaleott@andrew.cmu.edu,"Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/galeotti_john_2014.jpg,#N/A,"CV, Research Statement, and Teaching StatementI am a Senior Project Scientist and Adjunct Assistant Professor at Carnegie Mellon University (CMU), directing the Biomedical Image Guidance Laboratory and teaching an internationally recognized graduate course on medical image analysis algorithms. I have a Ph.D. in Robotics from CMU’s School of Computer Science, where my primary appointment is in the Robotics Institute (RI) and my adjunct (with advising privileges) is in Biomedical Engineering (BME). My B.S. and M.S. are in ECE, and I have an additional adjunct assistant professor appointment (also with advising privileges) in Bioengineering at the neighboring University of Pittsburgh (U. Pitt.).Through my interdisciplinary research, collaborations, and teaching experience, I have been continually working to improve patient outcomes by improving the tools of science and medicine, with an emphasis on applying novel, real-time computer-controlled optics, image analysis, and visualization approaches to build and control unique experimental systems for image-guided interventions, diagnosis, and biomedical research. Biomedical scientists have expressed specific interest in using my research to enable in-vivo measurements of nerve regrowth, to enable harvesting and transplantation of ocular stem cells, and to study yet inaccessible tissue biomechanics to better understand developmental morphogenesis and organogenesis, cancer dormancy and reemergence, and tissue engineering scaffold design.I am a significant contributor to the open-source Insight Segmentation and Registration Toolkit (ITK) for biomedical image analysis from the National Library of Medicine (NLM). I worked as part of the initial ITK Consortium, and I teach the longest continually running course in ITK, “Methods in Medical Image Analysis,” which is cross-listed in CMU’s RI, BME, and ECE departments, as well as at U. Pitt. For my teaching I have received external funding, two teaching awards, and the honor of mentoring another professor this spring. My public course website has been used as a basis for several similar classes around the world.My long-term goal is to invent fundamentally new optical imaging modalities to see things that have never been seen before, with corresponding breakthroughs in the analysis and visualization of such images.",http://www.ri.cmu.edu/person.html?person_id=1099,open http://www.ri.cmu.edu/person.html?person_id=1099
,Hartmut Geyer,hgeyer@andrew.cmu.edu,"Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/geyer_hartmut_2015_cropped.jpg,http://www.cs.cmu.edu/~hgeyer/,"My research focuses on legged systems. I work on principles of legged dynamics and control, their relation to human motor control, and resulting applications in rehabilitation technology. Please follow the link to my personal homepage for details about my research interests.",http://www.ri.cmu.edu/person.html?person_id=2355,open http://www.ri.cmu.edu/person.html?person_id=2355
,Ioannis Gkioulekas,igkioule@andrew.cmu.edu,"Assistant Professor, RI",Robotics Institute,,#N/A,#N/A,http://www.ri.cmu.edu/person.html?person_id=4447,open http://www.ri.cmu.edu/person.html?person_id=4447
,Abhinav Gupta,gabhinav@andrew.cmu,"Assistant Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/gupta_abhinav%20crop.jpg,http://www.cs.cmu.edu/~abhinavg,#N/A,http://www.ri.cmu.edu/person.html?person_id=2252,open http://www.ri.cmu.edu/person.html?person_id=2252
,Martial Hebert,mherbert@andrew.cmu.edu,"Director, Robotics Institute, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/hebert_martial.jpg,http://www.cs.cmu.edu/~hebert/,#N/A,http://www.ri.cmu.edu/person.html?person_id=109,open http://www.ri.cmu.edu/person.html?person_id=109
,Jessica K. Hodgins,jkh@andrew.cmu.edu,"Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/hodgins_jessica.jpg,http://www.cs.cmu.edu/~jkh,"My research focuses on the coordination and control of dynamic physical systems, both natural and human-made and explores techniques that may someday allow robots and animated creatures to plan and control their actions in complex and unpredictable environments while interacting with humans. To make progress toward this goal, I have active projects in computer graphics, human-robot interaction and humanoid robotics.My current computer graphics research focuses on generating human motion for computer animation. We have explored a number of techniques: using control algorithms in combination with physically realistic simulation, re-sequencing and interpolating motion capture clips, capturing skin and muscle deformations, and most recently data-driven simulations. In human-robot interaction, I am interested in determining which elements of human motion and behavior must be mimicked to facilitate natural interactions. In humanoid robotics, we are exploring control techniques for our Sarcos humanoid robot with the goal of producing natural looking motion.",http://www.ri.cmu.edu/person.html?person_id=851,open http://www.ri.cmu.edu/person.html?person_id=851
,Ralph Hollis,rhollis@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/hollis_ralph.jpg,#N/A,"My current research effort focuses on three main areas, all involving the creation of innovative new hardware, software, and systems. The first area concerns distributed agent-based cooperative high-precision manipulation. The principal application is agile assembly of small high-precision electromechanical products such as computer storage devices, medical devices, communication devices, and other high-density mechatronic equipment. The goal is to revolutionize the assembly of these kinds of products by drastically reducing the time it takes to design, program, and deploy automated assembly systems, while increasing their precision by several orders of magnitude and reducing their physical size. The second area concerns human-computer interaction, especially through haptic interaction with computed or remote environments. Here a goal is to enable truly transparent and high-fidelity interaction with eventual application to medicine, computer-augmented design, and telemanipulation, including scaled manipulation of microscopic and nanoscopic objects. The third area concerns intelligent mobile robots which are dynamically stable, including both rolling and walking machines. If such robots are to operate successfully in peopled environments, they must be agile and responsive to physical interaction with humans and their surroundings.In my experience, it is often very effective to synthesize novel robotic technology directly from physical principles, applying good engineering judgment rather than trying to build systems around collections of existing components. For this approach, a broad background including knowledge of physics, electrical and mechanical engineering, computer programming, and design is helpful. It is also extremely valuable to apply newly developed robot technology to real-world problems. Only in this way can one gain insight into the requirements for the technology.",http://www.ri.cmu.edu/person.html?person_id=117,open http://www.ri.cmu.edu/person.html?person_id=117
,Daniel Huber,huber@andrew.cmu.edu,"Senior Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/huber_daniel.jpg,http://www.cs.cmu.edu/~dhuber,"My area of expertise is three dimensional (3D) computer vision, specifically using high accuracy range sensors such as laser scanners for problems in the areas of modeling, recognition, and visualization. My goal is to make progress toward solving the problem of scene understanding using 3D computer vision. I am particularly interested in methods that combine image-based approaches from traditional computer vision with 3D computer vision algorithms. I am interested in methods to extract high-level semantics from 3D models, such as models of buildings. The ability to reverse engineer buildings has enormous potential benefit in a variety of fields, ranging from robotics to civil engineering to homeland security. I am also interested in methods for processing and visualizing 3D data in real time for high-speed teleoperation telepresence applications. Finally, I am studying the effects of sensor noise and data artifacts on the accuracy of 3D models for precision measurement applications.",http://www.ri.cmu.edu/person.html?person_id=123,open http://www.ri.cmu.edu/person.html?person_id=123
,Angel Jordan,aj0k@andrew.cmu.edu,"University Professor Emeritus, RI/ECE/ISR",Robotics Institute,http://www.ri.cmu.edu/images/people/angel.jpg,http://www.ece.cmu.edu/directory/details/73,"Over the years, I have been interested in and have conducted research on Semiconductor Devices, Integrated Circuits, Thin Films, Materials Science and Engineering, Gas Sensing Devices and Systems, Environmental and Biomedical Instrumentation, Intelligent Sensors, Advanced Video Systems, Technological Innovation, Management of Technology, and Studies of the Computer Industry. More recently I have been interested in Robotics, Automation, and Software Engineering, focusing on Technological Change, Technology Transfer, and Innovation.Software Technologies, Software Project and Process Management, Cyber Security, Software Outsourcing. Member of the Institute for Software Research, International.",http://www.ri.cmu.edu/person.html?person_id=133,open http://www.ri.cmu.edu/person.html?person_id=133
,Michael Kaess,kaess@andrew.cmu.edu,"Assistant Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/kaess_cmu.jpg,http://frc.ri.cmu.edu/~kaess/,"Perception is a fundamental challenge for mobile robots navigating through and interacting with their environment. My research focuses on 3D mapping and localization using information from any available sensor, including vision, laser, inertial, GPS and sonar (underwater). To enable online operation, my research also explores novel algorithms for efficient and robust inference at the intersection of linear algebra and probabilistic graphical models.",http://www.ri.cmu.edu/person.html?person_id=3210,open http://www.ri.cmu.edu/person.html?person_id=3210
,Takeo Kanade,kanade@andrew.cmu.edu,"U.A. and Helen Whitaker University Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/Takeo-Kanade-2016.jpg,http://www.cs.cmu.edu/~tk,"My research interests are in the areas of computer vision, visual and multi-media technology, and robotics. Common themes that my students and I emphasize in performing research are the formulation of sound theories which use the physical, geometrical, and semantic properties involved in perceptual and control processes in order to create intelligent machines, and the demonstration of the working systems based on these theories.My current projects include basic research and system development in computer vision (motion, stereo and object recognition), recognition of facial expressions, virtual(ized) reality, content-based video and image retrieval, VLSI-based computational sensors, medical robotics, and an autonomous helicopter.Computer visionWithin the Image Understanding (IU) project, my students and I are conducting basic research in interpretation and sensing for computer vision. My major thrust is the ""science of computer vision."" Traditionally, many computer vision algorithms were derived heuristically either by introspection or biological analogy. In contrast, my approach to vision is to transform the physical, geometrical, optical and statistical processes, which underlie vision, into mathematical and computational models. This approach results in algorithms that are far more powerful and revealing than traditional ad hoc methods based solely on heuristic knowledge. With this approach we have developed a new class of algorithms for color, stereo, motion, and texture.The two most successful examples of this approach are the factorization method and the multi-baseline stereo method. The factorization method is for the robust recovering of shape and motion from an image sequence. Based on this theory we have been developing a system for ""modeling by video taping""; a user takes a video tape of a scene or an object by either moving a camera or moving the object, and then from the video a three-dimensional model of the scene or the object is created. The multi-baseline stereo method, the second example, is a new stereo theory that uses multi-image fusion for creating a dense depth map of a natural scene. Based on this theory, a video-rate stereo machine has been developed, which can produce a 200x200 depth image at 30 frames/sec, aligned with an intensity image; in other words, a real 3D camera!!Currently, we are working on a rapidly trainable object recognition method, a system for modeling-by-video-taping, and a multi-camera 3D object copying/reconstruction method.Visual media technology for human-computer interactionA combination of computer vision and computer graphics technology presents an opportunity for a new exciting visual media. We have been developing a new visual medium, named ""virtualized reality."" In the existing visual medium, the view of the scene is determined at the transcription time, independent of the viewer. In contrast, the virtualized reality delays the selection of the viewing angle till view time, using techniques from computer vision and computer graphics. The visual event is captured using many cameras that cover the action from all sides. The 3D structure of the event, aligned with the pixels of the image, is computed for a few selected directions using the multi-baseline stereo technique. Triangulation and texture mapping enable the placement of a soft-camera to reconstruct the event from any new viewpoint. The viewer, wearing a stereo-viewing system, can freely move about in the world and observe it from a viewpoint chosen dynamically at view time. We have built a 3D Virtualized Studio using a hemispherical dome, 5 meters in diameter, currently with 51 cameras attached at its nodes.There are many applications of virtualized reality. Virtualized reality starts with a real world, rather than creating an artificial model of it. So, training can become safer, more real and more effective. A surgery, recorded in a virtualized reality studio, could be revisited by medical students repeatedly, viewing it from positions of their choice. Or, an entirely new generation of entertainment media can be developed - ""Let's watch NBA in the court"": basketball enthusiasts could watch a game from inside the court, from a referee's point of view, or even from the ""ball's eye"" point of view.A Virtualized Reality application, CBS's Eye Vision, was demonstrated during SuperBowl XXXV.Also, I am interested in and currently working on vision techniques for recognizing facial expression, gaze, and hand-finger gestures. Such techniques will provide natural non-intrusive means for human-computer interface by replacing current clumsy mechanical devices, such as datagloves.Informedia ProjectWith the growth and popularity of multimedia computing technologies, video is gaining importance and broadening its uses in libraries. Digital video libraries open up great potentials for education, training and entertainment; but to achieve this potential, the information embedded within the digital video library must be easy to locate, manage and use. Searches within a large data set or lengthy video would take a user through vast amounts of material irrelevant to the search topic. The typical database, which searches by keywords (e.g. title) - where images are only referenced and not directly searched for - is not appropriate or useful for the digital video library, since it does not provide the user a way to know the contents of the image, short of viewing it. New techniques are needed to organize these vast video collections so that users can effectively retrieve and browse their holdings based on their content. The Informedia Digital Video Library, funded by NSF, ARPA, and NASA, is developing intelligent, automatic mechanisms to populate the video library and allow for a full-content knowledge-based search, retrieval and presentation of video. The distinguishing feature of Informedia's approach is the integrated application of speech, language and image understanding technologies.Computational SensorWhile significant advancements have been made over the last 30 years of computer vision research, the consistent paradigm has been that a ""camera"" sees the world and a computer ""algorithm"" recognizes the object. I have been undertaking a project with Dr. Vladimir Brajovic that breaks away from this traditional paradigm by integrating sensing and processing into a single VLSI chip a computational sensor. The first successful example was an ultra fast range sensor which can produce approximately 1000 frames of range images per second an improvement of two orders of magnitude over the state of the art. A few new sensors are being developed including a sorting sensor chip, a 2D salient feature detector (2D winner-take-all circuits), and others.Medical Robotics and Computer Assisted SurgeryThe emerging field of Medical Robotics and Computer Assisted Surgery strives to develop smart tools to perform medical procedures better than either a physician or machine could alone. Robotic and computer-based systems are now being applied in specialties that range from neurosurgery and laparoscopy to opthalmology and family practice. Robots are able to perform precise and repeatable tasks that would be impossible for any human. The physician provides these systems with the decision making skills and adaptable dexterity that are well beyond current technology. The potential combination of robots and physicians has created a new worldwide interest in the area of medical robotics.We have developed a new computer assisted surgical systems for total hip replacement. The work is based on biomechanics-based surgical simulations and less invasive and more accurate vision-based techniques for determining the position of the patient anatomy during a robot surgery. The developed system, HipNav, has been already test -used in clinical setting.Vision-based Autonomous HelicopterAn unmanned helicopter can take maximum advantage of the high maneuverability of helicopters in dangerous support tasks, such as search and rescue, and fire fighting, since it does not place a human pilot in danger. The CMU Vision-Guided Helicopter Project (with Dr. Omead Amidi) has been developing the basic technologies for an unmanned autonomous helicopter including robust control methods, vision algorithms for real-time object detection and tracking, integration of GPS, motion sensors, vision output for robust positioning, and high-speed real-time hardware. After having tested various control algorithms and real-time vision algorithms using an electric helicopter on an indoor teststand, we have developed a computer controlled helicopter (4 m long), which carries two CCD cameras, GPS, gyros and accelerometers together with a multiprocessor computing system. Autonomous outdoor free flight has been demonstrated with such capabilities as following prescribed trajectory, detecting an object, and tracking or picking it from the air.*Selected Publications*       - *A Multi-body Factorization Method for Motion Analysis*   *J. Costeira and T. Kanade*   tech. report CMU-CS-TR-94-220, Computer Science Department, Carnegie    Mellon University, September, 1994. [Abstract]   Download: pdf [1456 KB], ps.gz [776 KB] copyrighted      - *A Paraperspective Factorization Method for Shape and Motion Recovery*   *C. Poelman and T. Kanade*   tech. report CMU-CS-93-219, Computer Science Department, Carnegie Mellon    University, December, 1993. [Abstract]   Download: pdf [448 KB], ps.gz [1760 KB] copyrighted      - *A Sequential Factorization Method for Recovering Shape and Motion    from Image Streams*   *T. Morita and T. Kanade*   *Proc. 1994 ARPA Image Understanding Workshop*, Vol. 2, November, 1994,    pp. 1177 - 1188. [Abstract]   Download: pdf [148 KB], ps.gz [461 KB] copyrighted      - *Development of a Video-Rate Stereo Machine*   *T. Kanade, H. Kato, S. Kimura, A. Yoshida, and K. Oda*   *Proc. of International Robotics and Systems Conference (IROS '95)*,    August, 1995.      Download: pdf [121 KB], ps.gz [260 KB] copyrighted      - *Development of a Video-Rate Stereo Machine*   *T. Kanade*   *Proceedings of 94 ARPA Image Understanding Workshop*, November, 1994.   copyrighted      - *Demo Program: Kanade-Okutomi Adaptive Window for Stereo Matcihing*,    July, 1997.   copyrighted      - *Picture Processing System by Computer Complex and Recognition of    Human Faces*   *T. Kanade*   doctoral dissertation, Robotics Institute, Carnegie Mellon University,    1974.      - *Video Skimming for Quick Browsing based on Audio and Image    Characterization*   *M. Smith and T. Kanade*   tech. report CMU-CS-95-186, Computer Science Department, Carnegie Mellon    University, July, 1995. [Abstract]   Download: pdf [2321 KB], ps.gz [4456 KB] copyrighted      - *Intelligent Access to Digital Video: Informedia Project*   *H. Wactlar, T. Kanade, M. Smith, and S. Stevens*   *IEEE Computer*, Vol. 29, No. 5, May, 1996, pp. 46-52.      - *Name-It: Association of Face and Name Video*   *S. Satoh and T. Kanade*   tech. report CMU-CS-96-205, Computer Science Department, Carnegie Mellon    University, 1996.      Download: pdf [380 KB], ps.gz [2290 KB] copyrighted      ",http://www.ri.cmu.edu/person.html?person_id=136,open http://www.ri.cmu.edu/person.html?person_id=136
,George A. Kantor,gkantor@andrew.cmu.edu,"Senior Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/kantor_george.JPG,http://frc.ri.cmu.edu/~kantor,"The control of dynamical systems becomes increasingly important as the era of robotics research dominated by quasi-static machines rapidly comes to a close. Similarly, the importance of state estimation grows as robotic applications require robots to function in larger, more complex environments. My research addresses both of these issues by focusing on the dual problems of controlling robotic mechanisms with non-trivial dynamics and perceiving the state of world through indirect measurements. My approach is both analytical and experimental: I use mathematics to understand the physical behavior of a given system and then use that understanding to create algorithms for control or estimation. I strive to develop new theoretical concepts and translate them into real-world implementations that solve problems such as balancing an unstable robot or estimating the location of an autonomous vehicle.",http://www.ri.cmu.edu/person.html?person_id=717,open http://www.ri.cmu.edu/person.html?person_id=717
,Alonzo Kelly,ak3h@andrew.cmu.edu,"Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/kelly_alonzo.jpg,http://www.frc.ri.cmu.edu/~alonzo,"My research agenda emerges from the struggle to develop mobile robot systems that are useful today in applications. In my case, “useful” means that the net effect on the profit and loss of the enterprise is a positive one. It fortunately happens to (still) be the case that trying to make profitable and/or cost-effective real systems presents enough fundamental challenges to keep researchers busy. Luckily, there is never a question about whether such efforts matter in the real world, because somebody patently cares enough to part with their hard-earned money to have them solved.So, what are those problems? My personal web site contains more details but some examples are provided below, organized by subsystems. These are often developed for either structured (indoor, road) or unstructured (off-road, off-earth) environments. The overall theme is each case is a fundamental problem of mobile robot systems was inspired from a specific application but thereafter attacked as generally as possible.Find more detailed information on my research interests here.",http://www.ri.cmu.edu/person.html?person_id=140,open http://www.ri.cmu.edu/person.html?person_id=140
,Kris M. Kitani,kmkitani@andrew.cmu.edu,"Assistant Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/kitani_kris.jpg,http://www.cs.cmu.edu/~kkitani,"computer vision, human activity analysis, first-person vision, assistive technologies, computational photography",http://www.ri.cmu.edu/person.html?person_id=2617,open http://www.ri.cmu.edu/person.html?person_id=2617
,Maxim Likhachev,mlikhach@andrew.cmu.edu,"Associate Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/likhachev_maxim2.jpg,http://www.cs.cmu.edu/~maxim,"My general research interests lie in Artificial Intelligence and Robotics.More specifically, they currently cover planning in deterministic and probabilistic domains and machine learning. My research has been mainly motivated by the problem of fast and intelligent decision making by autonomous robotic systems operating in real-world environments. Some of the robotic systems my group does planning for include mobile manipulators, aerial vehicles, multi-robot systems, and humanoid-like robots. I do get easily motivated, however, by other interesting planning problems.",http://www.ri.cmu.edu/person.html?person_id=1549,open http://www.ri.cmu.edu/person.html?person_id=1549
,Simon Lucey,slucey@andrew.cmu.edu,"Associate Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/lucey_simon%20crop.jpg,http://www.cs.cmu.edu/~slucey,"My research is motivated from a passion for discovering the ""why?"" behind ""how?"" with respect to core problems in Artificial Intelligence, Computer Vision and Machine Learning. I am currently leading the CI2CV laboratory, newly re-located to the Robotics Institute, Carnegie Mellon University in Pittsburgh, PA, USA where we are attempting to make theoretical and technological advancements in these topics. In particular, I am interested in three sub-fields of computer vision:- (i) Applying Filter Theory to Vision, (ii) Computer Vision on Mobile Devices, and (iii) Graph Based Structure From Motion. For more information on my research please refer to my personal web page.",http://www.ri.cmu.edu/person.html?person_id=1616,open http://www.ri.cmu.edu/person.html?person_id=1616
,Matthew T. Mason,mm3x@andrew.cmu.edu,"Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/mason_matt_2014.jpg,http://www.cs.cmu.edu/~mason/,"I work in robotics. The primary venue for my work is the Manipulation Lab (MLab).Simple HandsMy primary focus is simple hands. I think simple hands are better than complex anthropomorphic hands, both for research purposes and for most applications. The main reason is that the dexterity of a hand is mostly about the brain, not the hand. Our approach mixes a strong dose of machine learning into the traditional brew of AI, control and mechanical design. If you're curious, there are publications here. Probably the first one to read is the 2012 IJRR paper Autonomous Manipulation with a General-Purpose Simple Hand. Also see related papers from ICRA 2013 and RSS 2011, both of which won best student paper awards. The project began with NSF funding and is continuing with funds from NSF and the Army Research Office.Extrinsic DexterityIn robotic manipulation lingo, ""dexterity"" often refers to moving an object in the hand, like when you drop a coin from a fingertip grasp to your palm. However, most research on dexterity assumes a complex anthropomorphic hand. Under the standard assumption, a simple hand cannot shift grasps. But those assumptions are too restrictive. We have demonstrated the mechanical feasibility of dexterous manipulation using a simple hand. Our research is focused on going beyond mechanical feasibility, so that an autonomous robot could employ extrinsic dexterity by planning and controlling the simple hand for a given object and task. The first paper to read is the 2014 ICRA paper (Chavan Dafle et al.), perhaps followed by the Zhou et al. paper which won Best Conference Paper at ICRA 2016, or the upcoming WAFR 2016 paper by Hou et al.Manufacturing AutomationIndustrial automation is the ultimate proving ground for robotic manipulation research. This project, sponsored by Foxconn, is focused on automatic assembly of smart phones. The challenges are formidable, since smart phones use very small parts, some floppy and some sticky, and there is unavoidable clutter. We have a forthcoming paper surveying threaded fastening automation, and a recent paper on machine learning applied to classify stages of screw insertion (ISER 2016).",http://www.ri.cmu.edu/person.html?person_id=180,open http://www.ri.cmu.edu/person.html?person_id=180
,Nathan Michael,nmichael@andrew.cmu.edu,"Assistant Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/nathan-michael.jpg,http://nmichael.frc.ri.cmu.edu/,#N/A,http://www.ri.cmu.edu/person.html?person_id=2954,open http://www.ri.cmu.edu/person.html?person_id=2954
,Tom Mitchell,mitchell@andrew.cmu.edu,"E. Fredkin University Professor, CS",Robotics Institute,,http://www.cs.cmu.edu/~tom,"I am interested in artificial intelligence, especially in machine learning. Below are some research projects with opportunities for new students.Personal learning apprentices. This project aims to develop personalized software agents that help users perform specific tasks (e.g., manage their calendar, notice upcoming seminars of interest, reserve a meeting room), and that learn to improve throughout their lifetime by observing their users. We have recently developed one such learning apprentice: a personal calendar manager that helps schedule meetings, and that learns to provide advice about meeting dates, locations, durations, etc. This system is presently in routine use by one secretary, learns automatically each evening, and has successfully learned hundreds of simple rules for recommending appropriate durations and locations for meetings. New research is now needed to improve the underlying learning mechanisms, test the system's ability to learn from multiple users (we hope to distribute the system throughout SCS), and develop approaches for collaboration and colearning among multiple software agents.Learning robots. The goal of this work is to develop robots which learn from experience. Much of our recent research has focused on reinforcement learning: an inductive method for learning control strategies based on delayed reward. While we have succeeded in using such learning techniques to acquire control strategies for simple robot tasks (e.g., to dock on the battery charger), the most critical new research issue is to scale up these learning techniques to more complex and realistic robot learning scenarios. This will require new methods for learning to control robot sensors, adding hierarchical organization to the robot system, learning and utilizing background knowledge, etc. One active line of work in this area is a new learning method, described in the following paragraph.Explanation-based neural network learning. There are fundamentally two types of approaches to machine learning: inductive and analytical. Inductive learning methods (such as neural network backpropagation learning) find regularities by examining large numbers of training examples. Unfortunately, this approach requires huge numbers of training examples to learn complex functions. Analytical learning (such as symbolic explanation-based learning) uses prior knowledge of the learner to produce more correct generalizations from fewer examples. Unfortunately, this analytical approach requires substantial background knowledge, and is sensitive to errors in this background knowledge. We have recently developed an approach to unifying neural network and explanation-based learning approaches, in an attempt to combine the advantages of these complementary methods. New research is now needed to explore and extend this approach, and to try it out on problems such as robot learning.Learning to design. Within CMU's Engineering Design Research Center, a variety of intelligent design aids have been developed. Within this context, there are many opportunities to study the role of learning in design. For example, it may be possible to organize interactive design aids like MICON so that they learn new design techniques by observing their users and/or exploring large design spaces and evaluating resulting designs. I would like to find a student interested in learning and design, to start up a new project in this area.",http://www.ri.cmu.edu/person.html?person_id=190,open http://www.ri.cmu.edu/person.html?person_id=190
,Andrew Moore,awm@andrew.cmu.edu,"Professor and Dean, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/moore_andrew.jpg,http://www.cs.cmu.edu/~awm,"My research focuses entirely on applying machine learning to control. I want to find out just how autonomous we can make robots, factories and other complex control systems.A vast range of complex systems are becoming amenable to computer decision making. Most such systems have sensors, and so through their lifetime they experience a stream of sensor data. The machine analysis of this data stream can lead to the system improving its own internal knowledge of its behavior. And in turn it can use the improved knowledge to design improved control laws for itself. How can we do this mining of the data stream to extract as much predictive accuracy as possible, how can we do it quickly, and how can we then make the system develop good control laws with what it has learned?I work with these questions by developing a number of experimental algorithms, described shortly, and be testing them with a somewhat large variety of real-world problems. The large variety is to find out whether the algorithms are sufficiently autonomous that I can get away without being an expert in the application domains. Some completed, ongoing, and future applications include a billiards robot, robot juggling, candy bar manufacture, RC helicopters, decision making in economics, textile manufacture, RC toy cars, pinball, electricity pricing, car engine emission control and computer games.The kind of algorithms I work with lie at the borders between artificial intelligence, statistics, control theory and heuristic search. I make heavy use of a class of techniques called memory-based algorithms which have some useful properties. Predictions are made with interpolations from local previous experiences, and can be performed very quickly using geometric techniques, can circumvent the problems of a one-to-many inverse in redundant systems and avoid the necessity of retraining behaviors which have not been used recently.Another key to fast learning is exploration. Both the learning effort and the collection of data should be concentrated on the critical parts of the problem. Furthermore, real-world tasks can impose severe demands on learning controllers. I am developing the general memory-based learning (GMBL) system to tackle many of these difficulties. This technique centers on efficiently harnessing an amazingly powerful, but normally hopelessly expensive, statistical technique called leave-one-out cross-validation. This can help the systems automatically select the most appropriate function approximator, the best trade-off between fitting the data and filtering noise, and can be made to detect irrelevant, coupled or partially relevant sensors and actuators. Inventing good techniques to perform large-scale cross-validation searches is an exciting area with ongoing research opportunities.To generate improved control laws autonomously, the system must be able to make long-term predictions, and plan to achieve long-term goals. This planning must happen whilst simultaneously learning the environment. Weak-but-general search methods such as Dynamic Programming and A* search can prove helpful here, while retaining wide applicability to many domains. Some recent research involves scheduling search control during on-line planning to minimize wasted computations, and there are further interesting possibilities there. Ongoing research extends this to the PartiGame algorithm, in which exploration and planning are scheduled in a multi-resolution manner. A recursive partitioning of state-space adapts itself in real time to yield fine detail in the critical regions, while remaining at a coarse granularity elsewhere.I am also interested in working on several new developments of the research described above, ranging from (1) developing a quickly re-configurable meta-robot system with which we can produce, test and learn new control tasks with a turnover period of days instead of months, (2) developing formal theories of classes of control tasks, with associated proofs of convergence of appropriate machine learning algorithms, (3) developing new learning algorithms for certain classes of harder problems, and (4) tailoring efficient versions of certain optimization techniques, including genetic algorithms, to the control domain.",http://www.ri.cmu.edu/person.html?person_id=195,open http://www.ri.cmu.edu/person.html?person_id=195
,Igor Mordatch,igor.mordatch@gmail.com,"Assistant Professor (Joining in Fall 2017), RI",Robotics Institute,http://www.ri.cmu.edu/images/people/Igor-Mordatch-80-120.jpg,#N/A,#N/A,http://www.ri.cmu.edu/person.html?person_id=4399,open http://www.ri.cmu.edu/person.html?person_id=4399
,Jack Mostow,jmostow@andrew.cmu.edu,"Research Professor Emeritus, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/mostow_jack.jpg,http://www.cs.cmu.edu/~mostow,"My current interest is using computers to listen to children read aloud. The Reading Tutor adapts automated speech recognition to analyze oral reading. The Reading Tutor responds with spoken and graphical assistance modeled in part after expert reading teachers, but adapted to the strengths and limitations of the technology. Experimental use of the Reading Tutor in elementary school classrooms has produced gains in reading. My previous work in artificial intelligence included machine learning, automated replay of design plans, and discovery of search heuristics.Project LISTEN offers exciting opportunities for interdisciplinary research in speech and language technologies, cognitive and motivational psychology, human-computer interaction, computational linguistics, artificial intelligence, machine learning, graphic design, and of course reading.",http://www.ri.cmu.edu/person.html?person_id=201,open http://www.ri.cmu.edu/person.html?person_id=201
,Katharina Muelling,katharam@andrew.cmu.edu,"Systems Scientist, RI",Robotics Institute,"http://www.ri.cmu.edu/images/people/Muelling,%20Katharina.jpg",http://www.kmuelling.de,#N/A,http://www.ri.cmu.edu/person.html?person_id=3222,open http://www.ri.cmu.edu/person.html?person_id=3222
,Srinivasa G. Narasimhan,srinivas@andrew.cmu.edu,"Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/narasimhan_srinivasa_2015.jpg,http://www.cs.cmu.edu/~srinivas,"My research focuses on the physics of computer vision and computer graphics. My projects highlight three main aspects of my research - the mathematical modeling of the interactions of light with materials and the atmosphere; the design of novel cameras with higher resolution in space, color and intensity; and the development of algorithms for rendering and interpreting scene appearance. My research is motivated by applications in a wide range of fields including robotics, digital entertainment, remote sensing and underwater imaging.My research projects.",http://www.ri.cmu.edu/person.html?person_id=1371,open http://www.ri.cmu.edu/person.html?person_id=1371
,Illah Nourbakhsh,illah@andrew.cmu.edu,"Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/nourbakhsh_illah_02*.jpg,http://www.cs.cmu.edu/~illah,"For more than ten years I have been exploring human-robot interaction with the aim of creating rich, effective and satisfying interactions between humans and robots. My research has focused specifically on human-robot collaboration, wherein the robotic and human agents in the system share the same unifying goal or utility function. I further sharpen my scope to human-robot collaboration for learning, in which the measurable outcomes are information gain on the part of the humans in the system. In the context of my focus on collaboration for learning, rich means a cognitively sophisticated interaction in which humans and robots communicate as peers; effective means that formal measures of human learning should yield significant outcomes; satisfying means that humans should find the interaction both useful and pleasurable.Three key questions govern my inquiry into human-robot collaboration:One research focus has been to develop embedded solutions to the problem of semantic interpretation of events using visual sensing. Another focus has been robot navigation because it is an important prerequisite to many forms of social interaction when the robot shares the human physical space. For example visual-topological navigation and hybrid metric-topological models aim to provide navigation competency with a minimum of computational and memory demands. Because of the cross-disciplinary nature of the human-robot collaboration problem, integrative research must bring robotics together with other fields that model human cognition and social behavior. I have joined and extended models of interaction and evaluation methodology from Human Factors, HCI and Cognitive Psychology, outstanding complements to robotics since these fields already consider human relationships to physical embodiments and consider human behavioral change over time.Most recently we have begun to study the role of a research lab in meaningful design, dissemination and scaling with communities of practice. Our working model is one in which participatory design, design-based thinking and robotic innovation are combined to achieve positive social impact on specific problems throughout societies. For specific information about these projects, all of which are dedicated to make real impact while also establishing models for laboratory-community interaction, see my CREATE Lab website.",http://www.ri.cmu.edu/person.html?person_id=216,open http://www.ri.cmu.edu/person.html?person_id=216
,Stephen T. Nuske,snuske@andrew.cmu.edu,"Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/nuske_stephen_02.JPG,#N/A,"Full publication list and research information can be found at my personal website.My research interests are with the design of computer vision systems practical for field robotic applications. Computer vision, although a very promising perceptual tool, has been under-utilized in field robotics because of the difficulties creating reliable systems. It is the harsh nature of the environments that is especially challenging for computer vision systems. There are field robotic applications where computer vision is the only feasible perceptual mechanism due to its small size, low weight, long working range, and the rich nature of the information it provides. Therefore it is essential to keep researching ways to improve the performance of computer vision systems under challenging conditions in order to further progress the development of field robotics.My PhD work on visual localization - while I was at the CSIRO Autonomous Systems Lab and the University of Queensland, Australia - demonstrated that visual systems can in fact perform well in outdoor environments where historically lasers and other range sensors were thought to be the only possible sensory modalities for building reliable robotic systems.My current work at the Carnegie Mellon University - Field Robotics Center is on visual localization, visual tracking and visual scene and object segmentation for both small UAVs and ground-based agricultural vehicles.",http://www.ri.cmu.edu/person.html?person_id=2128,open http://www.ri.cmu.edu/person.html?person_id=2128
,Yong-Lae Park,ylpark@andrew.cmu.edu,"Assistant Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/park_yong-lae.jpg,http://www.cs.cmu.edu/~ylpark,"My research goal is to analyze the design and dynamics of biological systems and transform them into robotic/mechatronic systems for healthcare. For my Ph.D. research, I developed bio-inspired robots for surgical/interventional applications. During my postdoctoral years, I focused on designing artificial skin sensors and muscle actuators for soft wearable robots for rehabilitation and assistance.I am currently interested in bio-inspired design of soft robots and development of novel manufacturing methods for multi-material smart structures. Soft systems are a promising direction for future biomedical systems in which human-machine interactions are highly critical. The use of soft sensing and actuation materials and flexible electronics are imperative to build systems that both interact with and assist humans, as our bodies are covered with soft tissues. The target example systems include soft wearable rehabilitation/assistive devices, motion-sensing suits for whole body biomechanics, soft prosthetics and orthotics, soft surgical/interventional tools, and novel haptic interface devices.I envision my research being a foundation to establish a design and manufacturing methodology for developing soft robots by implementing biological sensing and actuation mechanisms.",http://www.ri.cmu.edu/person.html?person_id=3163,open http://www.ri.cmu.edu/person.html?person_id=3163
,Nancy Pollard,npollard@andrew.cmu.edu,"Associate Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/pollard_nancy.jpg,http://graphics.cs.cmu.edu/nsp/,"I am interested in understanding physical interaction with the environment --- how do we select and apply exactly the right forces to maneuver bulky and heavy objects, scramble over large rocks using both hands and feet, or use hand held tools?In robotics, a better understanding of these interaction forces can help us create more dexterous robots that are able to operate in an environment such as the home. In particular, we would like to create natural grasping and manipulation behavior using measured human examples as a resource. In initial experiments we have demonstrated a humanoid robot tumbling a variety of large, heavy objects using a strategy derived directly from a human example. Some of the questions that remain to be answered are ""what does it really mean for a robot to perform a task in the same way as a person?"", and ""how can we convert a collection of measured human examples into a robust control policy for a robot?""In computer graphics, an understanding of interaction forces can help us to create more natural looking motion when a character climbs, performs athletic maneuvers, or manipulates objects. We have developed fast techniques for computing optimal, physically plausible motion. We are also exploring the importance of physical correctness in graphics applications. How physically incorrect can motion be before people start to notice? In other words, how much can we cheat?One of my particular areas of interest in both robotics and graphics is the hand. Modeling convincing hand motion is very difficult; in fact the hand itself has almost as many degrees of freedom, or directions of motion as is typically used to model the entire rest of the body! However, observed motion of the hand often appears to be much less complex. By studying examples of human hand motion and studying human hand anatomy, we hope to characterize hand behavior in a way that can be exploited for easier control of animated hands and effective control of robot hands.",http://www.ri.cmu.edu/person.html?person_id=951,open http://www.ri.cmu.edu/person.html?person_id=951
,Deva Kannan Ramanan,deva@andrew.cmu.edu,"Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/ramanan_deva_crop.jpg,http://www.cs.cmu.edu/~deva/,#N/A,http://www.ri.cmu.edu/person.html?person_id=3421,open http://www.ri.cmu.edu/person.html?person_id=3421
,Raj Reddy,rr0s@andrew.cmu.edu,"Moza Bint Nasser University Professor, RI/CS/ISR",Robotics Institute,http://www.ri.cmu.edu/images/people/reddy_raj.jpg,http://www.rr.cs.cmu.edu,"Areas of interest: human-computer interaction, artificial intelligence, and rapid prototyping in manufacturing. Current active research projects include speech recognition and understanding systems, multimedia presentation technologies, collaborative writing, design, and planning, and the Automated Machine Shop project.",http://www.ri.cmu.edu/person.html?person_id=245,open http://www.ri.cmu.edu/person.html?person_id=245
,Cameron Riviere,criviere@andrew.cmu.edu,"Research Professor, RI/BioMed",Robotics Institute,http://www.ri.cmu.edu/images/people/riviere_cam.jpg,http://www.cs.cmu.edu/~camr,"Most of my research is in the area of surgical robotics. Specifically, I have an interest in developing robotic systems and interfaces for microsurgery and minimally invasive surgery that enhance the performance of the surgeon while also being ""minimally obtrusive:"" small, inexpensive, and easy to use, with minimal disturbance of the workflow in the operating room.Many of my projects deal with compensation of involuntary physiological motion for enhanced accuracy in surgery, using both active and passive approaches. One example of such a system is “Micron,” an active handheld micromanipulator which estimates the hand tremor of the surgeon and deflects its own tip with an equal but opposite motion to provide smooth control for specialties such as retinal microsurgery. This work has involved not only development of the robotic system, but also considerable effort in quantitative modeling of human tremor.Another example is “HeartLander,” a miniature crawling robot that passively compensates for heartbeat and respiratory motion by adhering to the surface of the heart using suction, while also crawling to specified targets on the heart and performing a variety of types of surgical intervention.My research involves building whole systems; thus, work in my lab includes modeling and simulation, mechanical design, electrical design, signal processing, control systems, fabrication and assembly, and testing with human users.As an outgrowth of the tremor modeling research, I also do research in filtering of input signals to provide assistive computer interfaces for persons with movement disorders including athetoid cerebral palsy and a variety of types of pathological tremor, in order to enable improved control of devices such as personal computers and powered wheelchairs.",http://www.ri.cmu.edu/person.html?person_id=248,open http://www.ri.cmu.edu/person.html?person_id=248
,Sebastian Scherer,basti@andrew.cmu.edu,"Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/scherer_sebastian.JPG,http://www.scherers.net,#N/A,http://www.ri.cmu.edu/person.html?person_id=1397,open http://www.ri.cmu.edu/person.html?person_id=1397
,Jeff Schneider,jeff4@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/schneider_jeff_2014.jpg,http://www.cs.cmu.edu/~schneide,"My research is in active learning, data mining,reinforcement learning, optimization, and intelligent control. My efforts are focused on the application of these methods to real world industrial and commercial problems.",http://www.ri.cmu.edu/person.html?person_id=269,open http://www.ri.cmu.edu/person.html?person_id=269
,Yaser Ajmal Sheikh,yaser@andrew.cmu.edu,"Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/sheikh_yaser.jpg,http://www.cs.cmu.edu/~yaser/,"I am interested in analyzing dynamism in scenes from moving cameras. In particular, my research focuses on dynamic motion reconstruction, human behavior analysis, estimation of nonrigid motion and modeling moving cameras in spacetime.",http://www.ri.cmu.edu/person.html?person_id=1664,open http://www.ri.cmu.edu/person.html?person_id=1664
,Mel Siegel,mws@andrew.cmu.edu,"Associate Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/siegel_mel_2014.jpg,http://www.cs.cmu.edu/~mws,"My research interests originate in parallel backgrounds of measurement intensive basic research in physical science, commercial analytical instrument development, application of computers to measurement, diagnosis, and control, and computer modeling and design of measurement and perception systems. My research in robotics has focused on the design and integration of sensing devices, measurement and control systems, and knowledge based approaches to interpretation and utilization of data. Past projects have addressed sensors and algorithms for biotechnology process control, interferogram based fiberglass forming control, tactile sensor based manipulator control, development semiconductor devices that simulate the olfactory sense, and detection and analysis of electric power transmission system faults using expert systems. Current and future research directions are illustrated by the following project capsules. Their unifying theme is making ""difficult measurements in difficult environments"", seeking new ways to formulate and execute control decisions using incomplete, noisy, ambiguous and contradictory measurements.Mobile Robots for Aging Aircraft Inspection. With each takeoff and landing, the skin of a pressurized airplane expands and contracts, causing cracks around the rivet heads. Load induced stresses cause cracks deep in structural members, engine components, etc. Weather and the environment lead to dangerous weakening of bonds and structures by corrosion. Present inspection procedures to find these flaws are about 10% electronically instrumented, primarily using hand-held eddy current sensors, and about 90% visual, aided only by off-the-shelf magnifiers and flashlights. We have two projects in which we are studying the four key issues in the application of mobile robots to aiding and perhaps eventually automating these inspections. The first project focuses on the mobility and manipulation issues in electronic measurement instrument deployment; it uses computer vision to aid navigation, alignment, and motion control of a small suction cup based beam-walker that we designed and built. The second project focuses on the measurement and monitoring issues of visual inspection, especially camera, software, and display systems for doing remote enhanced visual inspection using 3D-stereoscopic methods; in this project a large but very light weight wheeled robot is being designed and built to systematically scan the fuselage crown of large passenger and cargo airplanes. We also contemplate a future microrobotics program for inspection of inaccessible areas such as fuel tanks, wheel wells, and the inside surface of the skin.3D-Stereoscopic Display Systems. We are working on several key obstacles to 3D-TV and 3D-stereoscopic computer graphics systems successfully making the transition from currently cumbersome laboratory curiosities to eventually standard appliances in the home and the workplace. Research issues that we are working on include camera and graphics engine design, binocular and multi-perspective signal stream compression and coding, 3D-stereoscopic display hardware implementations that are cognizant of and able to exploit the psychophysics of binocular perception, and efforts to quantify the additional utility of 3D- stereoscopy for perceiving complex data, virtual reality, and intricate spatial relationships. We are carefully examining the geometrical and optical system issues that underlie virtual reality presentations of naked-eye, augmented- eye (binoculars, stereomicroscopes), and fish-eye perspectives. We are able to couple sensing of the observer's position to the display's viewpoint, suitably changing the perspective from which computer graphics are rendered, and suitably interpolating camera viewpoints (and potentially moving selected cameras) in multi-viewpoint video systems. This work is coupled with our aircraft inspection work via incorporation of advanced 3D- stereoscopic cameras and display systems in the enhanced remote visual inspection sensor package. We are also beginning to apply this work to some issues in medical robotics and medical imaging, including dynamic fusion of imagery from x-ray CT, ultrasonic, and video sources, and measurements for optimizing and manufacturing custom designed seat cushions for wheelchair bound patients.Modeling of Illumination Systems. During the course of a project to generate simulation and design aids for a manufacturer of automobile head-lamps and tail-lamps we conceived a new approach to modeling complex illumination systems. The method combines automated systematic measurements that fully characterize the radial dependence of the angular distributions of light from extended light sources with a parametric representation akin to the multipole expansions used in antenna and atomic radiation models. Using this approach, we have succeeded in achieving very compact representations of complex off-the- shelf light sources from which raytracing can proceed extremely efficiently. We have used this representation to model luminaires, and to optimize their reflector shapes to achieve desired complex lighting patterns such as those required of automobile head lamps. We have also developed an encapsulation of the key angular features of light sources in a small number of coordinate system independent parameters, thus making it possible to compare and contrast different sources, and even different types of sources, on an equal footing.Sensor Device Oriented Projects. We often invest a little time and a few spare dollars to pursue an innovative concept, usually an idea for a sensor device, by building a laboratory demonstration of it potential. We have a unique approach to range-from- focus: harmonic analysis of the signal from an oscillating image sensor gives the focus error, and thus the object range, pixel-by-pixel, without explicit need to focus any part of scene. We build semiconductor gas sensors, devices that change an electrical property in response to trace constituents in the environment, using neural networks to model and replicate the sense of smell. We are using electrets, waxy materials cast or molded into finger-like shapes and electrically polarized to make them pressure sensitive, for tactile sensors in delicate inspection and manipulation tasks. Ideas like these are good ones for student projects and, when developed in depth, can make good thesis research topics.",http://www.ri.cmu.edu/person.html?person_id=285,open http://www.ri.cmu.edu/person.html?person_id=285
,Daniel Siewiorek,ds1p@andrew.cmu.edu,"Professor, RI/HCII",Robotics Institute,http://www.ri.cmu.edu/images/people/siewiorek_dan.jpg,http://www-2.cs.cmu.edu/~dps/,#N/A,http://www.ri.cmu.edu/person.html?person_id=286,open http://www.ri.cmu.edu/person.html?person_id=286
,Reid Simmons,rsimmons@andrew.cmu.edu,"Research Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/simmons_reid.jpg,http://www.cs.cmu.edu/~reids,"My research interests focus on developing reliable, highly autonomous systems (especially mobile robots) that operate in rich, uncertain environments. The goal is to create intelligent systems that can operate autonomously for long periods of time in unstructured, natural environments. This necessitates robots that can plan, effectively reason about uncertainty, diagnose and recover from unanticipated errors, and reason about their limitations. In particular, I am interested in architectures for autonomy that combine deliberative and reactive behavior, reliable execution monitoring and error recovery, multi-robot coordination, probabilistic and symbolic planning, formal verification of autonomous systems, and human-robot social interaction.",http://www.ri.cmu.edu/person.html?person_id=289,open http://www.ri.cmu.edu/person.html?person_id=289
,Sanjiv Singh,js89@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/singh_sanjiv.jpg,http://www.frc.ri.cmu.edu/~ssingh/,"Autonomous Navigation. This theme is developing motion planning, state estimation and real-time control for ground and air vehicles at various scales. Applications include agriculture (CASC project), exploration with air vehicles (Riverine project) and autonomous low flying aircraft (Transformer project)Coordinated Multi-Robots. This theme examines tasks such as assembly of structures that can't be conducted by single robots. We are developing motion planning and architectures that enable teams of robots and humans to operate efficiently and reliably. Applications include assembly of structures (Trestle and Cluster projects) and coordinated search/localization in first response (Ember project)Forceful Interaction with the world. I am interested in a class of tasks in which the interaction between a robot and the world is complex and involves large forces. The approach is to select optimal actions using physical models of the interaction, thereby making the machines more capable.",http://www.ri.cmu.edu/person.html?person_id=290,open http://www.ri.cmu.edu/person.html?person_id=290
,Stephen Smith,ssmith@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/smith_steve_2014.jpg,http://www-2.cs.cmu.edu/~sfs/,#ERROR!,http://www.ri.cmu.edu/person.html?person_id=293,open http://www.ri.cmu.edu/person.html?person_id=293
,Siddhartha Srinivasa,ss5@andrew.cmu.edu,"Finmeccanica Associate Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/srinivasa_sidd_2014.jpg,http://www.cs.cmu.edu/~siddh,"<p>I work on manipulation. My goal is to enable robots to robustly and gracefully interact with the world to perform complex manipulation tasks in uncertain, unstructured, and cluttered environments. I want to make this interaction faster, safer, elegant, and involve simpler actuation. To this end, I founded and direct the <a href='http://personalrobotics.ri.cmu.edu'>Personal Robotics Lab</a>, co-direct the <a href='http://www.cs.cmu.edu/~mlab'>Manipulation Lab</a>, and lead the HERB effort of the QoLTbots systems area and the Mobile Manipulation effort of the Mobility and Manipulation Thrust at the <a href='http://www.qolt.org'>Quality of Life Technologies NSF ERC</a>.</p>
<p class='cmsP'><p>I am currently focussing on two topics: <b>Physics-based Manipulation</b> and <b>The Mathematics of Human-Robot Interaction</b>. They are heavily intertwined, both born out of the goal of robots performing complex manipulation tasks with and around people.</p>
<p class='cmsP'><p><b>Physics-based Manipulation:</b><br />
I focus on using physics in the design of actions, algorithms, and hands for manipulation:</p>
<p class='cmsP'><ul>
<p class='cmsP'><li>Manipulation is more than pick-and-place. We are developing <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6652'>nonprehensile physics-based actions</a> and algorithms to <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7080'>reconfigure clutter</a> in the way of the primary task by pulling, pushing, sweeping, and sliding it out of the way. We have also recently shown that a class of tactile localization problems can be formulated to be <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7264'>submodular</a>.</li>
<p class='cmsP'><li>I am particularly fond of functional gradient methods which have been used with much success in <a href='http://en.wikipedia.org/wiki/Path_integral_formulation'>physics</a>. We have developed <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6285'>CHOMP</a>, a functional gradient optimizer for robot motion planning, and variants like <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6810'>GSCHOMP</a> that exploit the structure of manipulation problems.</li>
<p class='cmsP'><li>I believe simple hands can do complex things. I am working on building robustness into the design and algorithms of <a href='http://www.ri.cmu.edu/research_project_detail.html?type=description&project_id=661&menu_id=261'>simple hands</a>, embracing physics and underactuation to stabilize objects.</li>
<p class='cmsP'></ul>
<p class='cmsP'></p>
<p class='cmsP'><p><b>The Mathematics of Human-Robot Interaction:</b> I focus on formalizing Human-Robot Interaction principles using machine learning, motion planning, and function gradient algorithms:</p>
<p class='cmsP'><ul>
<p class='cmsP'><li>We have been working on enabling seamless and fluent human-robot handovers. We have developed a taxonomy of <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6789'>human and dog handovers</a>, designed expressive <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6925'>grasps</a> and <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=6793'>motions</a>, and used time-series analysis to learn the <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7258'>communication of intent</a>. Our <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7389'>JHRI paper</a> summarizes this work. </li>
<p class='cmsP'><li>We are <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7077'>formalizing assistive teleoperation</a>, framing it as the joint tightly-coupled problem of prediction, solved with machine learning, and arbitration by policy blending, solved with control theory. We are also working on the <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7224'>online adaptation of teleoperation interfaces</a> with kernel machines.</li>
<p class='cmsP'><li>Our latest work formalizes <a href='http://www.ri.cmu.edu/publication_view.html?pub_id=7400'>predictable and legible motion</a> as inference problems, bringing together concepts in psychology, animation, and machine learning. We are presently working on generating legible motion via functional gradient optimization.</li>
<p class='cmsP'></ul>
<p class='cmsP'>I have greatly enjoyed collaborating with HRI researchers, and our joint exploration has broadened my understanding of humans, robots, and mathematics.
</p>
<p class='cmsP'><p> I am also interested in <b>Manipulation Planning</b>:<br />
<a href='http://personalrobotics.ri.cmu.edu/projects/cplan.php'>extending randomized planners to constraint manifolds</a> and <b>Perception for Manipulation</b>: my group has developed <a href='http://personalrobotics.ri.cmu.edu/projects/moped.php'>MOPED: an efficient object recognition and pose estimation system for manipulation</a>.
</p>",http://www.ri.cmu.edu/person.html?person_id=696,open http://www.ri.cmu.edu/person.html?person_id=696
,Aaron Steinfeld,as7s@andrew.cmu.edu,"Associate Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/steinfeld_aaron.jpg,http://www-2.cs.cmu.edu/~astein/,"<p class='cmsP'>My specialty is <b>human-robot interaction</b>. The central themes of my work are what I prefer to term <i>constrained user interfaces</i> and <i>operator assistance</i>. I am interested in how to enable timely and appropriate interaction when interfaces are restricted through design, tasks, the environment, time pressures, and/or user abilities. I utilize training and experience in human factors and ergonomics, robotics, intelligent transportation, rehabilitation, human-computer interaction, universal design, wearable computing, and research methods. My two core areas of interest are:</p>
<p class='cmsP'><b>Human-Robot Intent Fusion:</b> A system level view is important for ensuring task success through a mixture of human adaptability and robot precision and perseverance.</p>
<p class='cmsP'><b>Appropriate Robot Behavior:</b> End users expect appropriate robot actions, interventions, and requests for human assistance.</p>
<p class='cmsP'>I have a strong interest and extensive experience with design and  execution of evaluation experiments for complex human-in-the-loop  systems. Examples of past work includes human-machine interaction and  interfaces for collision warning systems, drowsy driving, zero- visibility snowplow operations, advanced in-vehicle systems, head-up  displays, real-time captioning, rehabilitation robotics, military  robotics, semi and fully autonomous mobile robots, and software agents.
</p>",http://www.ri.cmu.edu/person.html?person_id=982,open http://www.ri.cmu.edu/person.html?person_id=982
,Anthony (Tony) Stentz,stentz@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,"http://www.ri.cmu.edu/images/people/Stentz,%20Anthony.jpg",#N/A,"My research goal is to automate large, mobile equipment operating outdoors in unstructured environments. Examples include excavators and wheel loaders used in surface mining, windrowing machines and combines used in agriculture, continuous miners and haulage systems used in underground mining, and unmanned ground vehicles employed in military scouting operations. For the commercial machines, automation promises to increase productivity, improve safety, and reduce operational costs.<br />
<P></p>
<p class='cmsP'>My focus is planning and perception for robots in these applications. In unstructured environments, planning is difficult due to uncertainty. Typically, the robot does not have full information about its environment before it produces a plan. Sensors are able to fill in the missing information as the plan executes, but sensor data itself can be inaccurate. Even with perfect information, robot actions can have unexpected consequences. I investigate techniques for rapidly replanning robot operations in the presence of these types of uncertainty.<br />
<P></p>
<p class='cmsP'>The perception problems for these applications are also challenging. The robot's environment contains irregular geometry that is difficult to represent and manipulate, such as undulating terrain, mine walls, and soil banks. Natural features, such as plants, rocks, and water puddles, combine with man-made objects to create a rich and complex environment. Adverse weather can impair the ability of the robot to sense and interpret its environment. Objects of interest may be buried beneath the soil or hidden behind a rock face. The environment can change over time as the robot interacts with it. I investigate sensors and processing techniques that can handle these conditions and extract the necessary information from the environment to perform the task.<br />
<P></p>
<p class='cmsP'>Current projects include autonomous truck loading using excavators and wheel loaders, mining of coal, harvesting of alfalfa and other crops, and navigation of reconnaissance vehicles.<br />
<P></p>
<p class='cmsP'><B>Autonomous Loading System (ALS). </B>The goal of this project is to develop technologies to automate mass excavation with excavators and wheel loaders. In a typical mass excavation scenario, a loading machine digs material from a face and dumps the material in a truck, with a throughput of hundreds of trucks per day. The process is repetitive and continues day and night and in most weather conditions. The type of loading machine and truck can vary, as can the material to be loaded which may range from sand to hard rock. Other issues include maintaining safety of all machines and personnel in the loading area and coordination of multiple machines to achieve a prescribed goal.<br />
<P></p>
<p class='cmsP'>Previous project work in excavation focused on the retrieval of buried objects such as ordnance or hazardous waste. The goal was to automate the task to eliminate the hazard to humans. The system used a ground penetrating radar mounted on a robotic arm to recognize and localize buried objects. Once identified and characterized, the robotic arm was able to automatically remove the overburden and retrieve the object.<br />
<P></p>
<p class='cmsP'><B>Autonomous Coal Mining (USBM).</B> The goal of this project is to automate continuous mining equipment. Thus far we have developed a complete navigation system for a continuous mining machine, including laser rangefinder-based perception for robot localization, planning for cluttered spaces, and integration and simulation tools. This system was successfully demonstrated in a real mine in West Virginia. Future work includes automated mine surveying, haulage, multiple-machine interaction, and the commercialization of these technologies.<br />
<P></p>
<p class='cmsP'><B>Autonomous Harvesting (DEMETER).</B> The goal of this project is to automate the cutting of alfalfa and other crops using a harvesting machine. DEMETER is a NewHolland 2550 Windrower retrofitted for computer control of the speed and steering functions. DEMETER is equipped with a CCD camera and on-board computer for visually detecting the line between cut and uncut crop and steering the machine to cut a crop row. DEMETER is capable of detecting the end of the row and turning to cut the next row. The machine can detect and stop for obstacles in its path. To date, DEMETER has cut alfalfa on a farm in Western Pennsylvania at a maximum speed of 4.5 mph. By the end of 1996, we plan to cut 100 acres of crop fully autonomously in California.<br />
<P></p>
<p class='cmsP'><B>Unmanned Ground Vehicle (UGV).</B> The objective of this project is to automate navigation for a military scouting vehicle. Our research vehicle, NAVLAB II, is a computer-controlled HMMWV equipped with a scanning laser rangefinder and stereo cameras for obstacle detection. For cross-country navigation, our work has focused on two capabilities: local obstacle avoidance and global navigation. We have developed a system that drove the NAVLAB II over 10 kilometers continuously while avoiding obstacles. Another system drove the NAVLAB II a distance of 1.4 kilometers to a goal location in a cluttered environment, given no initial map information. The system produced an obstacle map of the terrain it explored en route to the goal.
</p>",http://www.ri.cmu.edu/person.html?person_id=299,open http://www.ri.cmu.edu/person.html?person_id=299
,George D. Stetten,stetten@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/stetten_george_2014.jpg,http://www.stetten.com,"Although an MD/PhD, I am primarily an engineer, with extensive background in electronics and computer science, the development of new technology and prototype design.  My current research focuses on two main areas.  The first involves developing a new method of image guided intervention based on in-situ image guidance.  One example of this approach is a device called the Sonic Flashlight, which provides a stable virtual image of an ultrasound scan within the patient in real time.  Over the past 8 years of continuous NIH funding, my laboratory has developed and tested the device in patients for the placement of Peripherally Inserted Central Catheter (PICC) lines into the upper arm.  Extensions of this technology to remote and magnified image guided intervention have also been developed, as well as a version based on a holographic optical element that is currently funded by NIH/NIBIB.  Yet another version uses Real Time 3D ultrasound, an imaging modality that I helped to develop at Duke University in the 1990’s.  Our most recent effort over the past two years has been to extend in-situ imaging into the domain of microsurgery using Optical Coherence Tomography, which is the topic of the current proposal.</p>
<p class='cmsP'>My second research area is in image analysis techniques for automated identification and measurement of anatomical structure and motion.  Recent applications include cardiac structures, the major veins of the neck and the vasculature of the lung. Image analysis was the area of my Ph.D. dissertation with Stephen Pizer at UNC, Chapel Hill.  I was a founding member of the National Library of Medicine (NLM) software consortium creating the <a href='www.itk.org'>Insight Toolkit</a> (ITK), an open-source software library for medical image analysis.</p>
<p class='cmsP'>In addition to these two main areas, I am exploring a new prosthetic device for the blind called fingersight, and am involved in electronic musical instrument design and other projects in the context of teaching undergraduates in Bioengineering. My earlier experiences in engineering include writing the software for the first computer system onboard Deep Submersible Alvin at the Woods Hole Oceanographic Institute and designing a telemetric egg to study incubation of endangered birds at the Bronx Zoo.",http://www.ri.cmu.edu/person.html?person_id=677,open http://www.ri.cmu.edu/person.html?person_id=677
,Katia Sycara,sycara@andrew.cmu.edu,"Research Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/sycara_katia.jpg,#N/A,"Software agents and robots are becoming increasingly part of human lives. For example, mobile sensors are being deployed for a variety of military and civilian applications. These  robots are for the most part teleoperated. We envision a future where large numbers of networked  humans and robots will be working together for numerous applications ranging from manufacturing to environment exploration, crisis response and search and rescue. These multi-robot systems will be coordinating autonomously, and in many cases will be interacting with humans in a variety of interactions. I am interested in both (a) Developing algorithms and techniques for <b>Autonomous Multi-Robot Coordination</b>, and (b) developing a variety of techniques for <b>Human Single- and Multi-Robot Interactions</b>.<br />
<br><br />
In the area of  Autonomous Multi-Robot Coordination, I have particular interest and considerable amount of research in developing algorithms for decentralized task allocation with formal performance guarantees, negotiation  algorithms for resolution of conflicts with multiple issues and algorithms for adversarial interactions.  The algorithms take into consideration the dynamics of the environment, uncertainty and risk. These algorithms have wide applicability ranging from manufacturing, search and rescue, to package handling and transportation.<br />
<br><br />
In the area of Human-Robot Interaction (HRI), I am working in developing a formal framework that models <b>HRI from the perspective of computational complexity</b> (constant, linear, exponential) of the human’s control task and needed interaction with the robots. This allows a systematic study of human interaction, adjustable autonomy, human-robot teaming and control with multiple robots. This framework has enabled us to develop effective algorithms for scheduling operator attention, and also discover a new concept, <b>Neglect Benevolence</b>, namely that it may be beneficial for a robotic swarm to be neglected for some amount of time before the human operator provides the next input, as mission goals change.  Additionally, I am working towards the development of <b>cognitively-based analytical models</b> of the human operator so that the overall human-robot system can be formally verified.<br />",http://www.ri.cmu.edu/person.html?person_id=304,open http://www.ri.cmu.edu/person.html?person_id=304
,Matthew J. Travers,mtravers@andrew.cmu.edu,"Systems Scientist, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/travers_matt.jpg,https://matthewtravers.org,#N/A,http://www.ri.cmu.edu/person.html?person_id=2701,open http://www.ri.cmu.edu/person.html?person_id=2701
,Howard Wactlar,wactler@andrew.cmu.edu,"Alumni Research Professor, RI/CS",Robotics Institute,http://www.ri.cmu.edu/images/people/wactlar_howard.jpg,http://www.cs.cmu.edu/~hdw/,#N/A,http://www.ri.cmu.edu/person.html?person_id=329,open http://www.ri.cmu.edu/person.html?person_id=329
,Lee Weiss,lew@andrew.cmu.edu,"Research Professor Emeritus, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/weiss_lee_2015.jpg,#VALUE!,#VALUE!,,open 
,David Wettergreen,dw0s@andrew.cmu.edu,"Research Professor; Associate Director for Education and Director of the 
Ph.D. Program, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/wettergreen_david.jpg,http://www.frc.ri.cmu.edu/~dsw,#N/A,http://www.ri.cmu.edu/person.html?person_id=638,open http://www.ri.cmu.edu/person.html?person_id=638
,William (Red) L. Whittaker,ww0t@andrew.cmu.edu,"University Professor, RI",Robotics Institute,http://www.ri.cmu.edu/images/people/whittaker_red.jpg,http://www.frc.ri.cmu.edu/users/red/,"My research interests center on mobile robots in unpredictable environments, such as natural terrain and outdoor worksites, including computer architectures to control mobile robots, modeling and planning for non-repetitive tasks, complex problems of objective sensing in random or dynamic environments, and integrations of complete field robot systems.<br />
<P></p>
<p class='cmsP'><b>Remote Reconnaissance & Inspection</b></p>
<p class='cmsP'>The first machines that initiated “field robotics” (1982) addressed a need to remotely inspect the aftermath of a nuclear accident at Three Mile Island. The Remote Reconnaissance Vehicle brought back the first footage from the flooded basement of the damaged reactor. The Remote Core Sampler returned samples of the walls and the Remote Work Vehicle was built to do a large number of tasks inside the basement. Pioneer (1998), a mobile mapping and reconnaissance machine for structural assessment was deployed at the damaged Chernobyl nuclear power plant. Tesselator (1992) was developed for automated inspection and waterproofing of the tiles on the Space Shuttle. Tugbot (2006) surveyed an 1800-acre site in Nevada for hazards and buried objects.<br />
<P></p>
<p class='cmsP'>My work encompasses core research, prototyping, and experimentation with the view that all are important to the evolution of field robots. Increasingly, my research interests are manifested through the work of the Field Robotics Center (FRC), which I direct. I have particular agenda in integrating component technologies into complete systems that prove themselves in both research and real world contexts. At FRC we developed the remote work systems that explored and remediated the basement of the crippled Three Mile Island reactor containment basement. The Remote Reconnaissance Vehicle performed recovery tasks such as inspection, radiological mapping, material sampling, sludge transport and wall cleaning in a highly radioactive environment. Its successor, the <B>Remote Work Vehicle</B> (RWV), a telerobot of unprecedented capability and nuclear qualification, was developed for a broad agenda of clean-up operations. The RWV can wash contaminated surfaces, remove sediments, demolish radiation sources, apply surface treatments, and package and transport materials.<br />
<P></p>
<p class='cmsP'><b>Outdoor Navigation</b></p>
<p class='cmsP'>Terregator (1983) was one of the earliest outdoor mobile robots and served as a testbed for much of the early work in autonomous road following and mine modeling. Its successor, Navlab (1985), a converted van, was the first full sized vehicle to house researchers and computing and served as a testbed for unmanned ground vehicle research. Navlab founded a series of autonomous outdoor vehicles with a variety of configurations.<br />
<P></p>
<p class='cmsP'>The FastNav project investigated the areas of path tracking and collision avoidance for robot vehicles in featureless terrains, such as strip mines and hazardous waste sites. Collision avoidance was accomplished by searching a section of the path ahead of the vehicle using laser ranging. To guarantee collision avoidance, it is necessary to guarantee detection by the sensor and real time response from the computing and vehicle actuators. Path tracking was accomplished by servoing to a specified path using sophisticated inertial guidance. To date we have successfully demonstrated path tracking on the NavLab at 25 km per hour using a kinematic model of the vehicle. In the future we plan to extend our control schemes to track paths at higher speeds by incorporating vehicle dynamics and to detect obstacles on undulating terrain.<br />
<P></p>
<p class='cmsP'>From sensor research in subsurface investigation, we developed the <B>Portable Pipe Mapper (PPM)</B>, a hand-held pipe mapping system that collects, enhances, and displays a gray-scale map of buried ferrous pipes. The PPM gives the user a powerful tool for inferring utility line location through a visual and spatial representation of sensor data. It displays elbows, Ts, and crosses in piping networks and provides an accurate depth estimation of a target pipe.<br />
<P></p>
<p class='cmsP'>We are developing a <B>Site Investigation Robot</B> to increase the efficiency of hazardous waste site investigations by integrating automated data acquisition, advanced subsurface sensing, robotic positioning, and site data basing through a uniform user-friendly interface. We have automated ground penetrating radar (GPR) scanning and data acquisition, using robotic technology to provide high accuracy for signal processing. Our work in three-dimensional image processing matches the three dimensional potential of GPR, achieving higher resolutions and full-volume imaging. Ultimately, a field technician will be able to scan a site and determine the type and locations of objects for excavation.<br />
<P></p>
<p class='cmsP'>Our research into the automation of subsurface mining machines, specifically in the area of navigation, motion safe-guarding and position registration, has demonstrated corridor following using our Terregator equipped with sonar and laser range sensors. Current research is supported by the Locomotion Emulator (LE), an omnidirectional locomotion testbed. The LE's distinction as a testbed lies in its ability to emulate a wide variety of target vehicles, of which mining machines are examples, through software reconfiguration. The success of the LE is measured in part by the extent to which software for a target application, developed on the LE, ports to end use with minimum modification.<br />
<P><br />
<b>Robotic Exploration</b></p>
<p class='cmsP'>Ambler (1987), a novel walking robot intended for a Mars sample-return mission, was built to navigate over large obstacles while operating on very low power. Skyworker (2002), is a similarly motivated robot that walks on the trusses of space structures. Dante I & II (1992, 1994) were unmanned robots for volcano exploration deployed at Mt. Erebus, Antarctica and Mt. Spurr, Alaska. Nomad (1996) is a novel robot mechanism designed to operate in rough terrain. It was first extensively deployed in the Chilean Atacama Desert in 1997 and was subsequently used for the first robotic search for Antarctic meteorites (2000). A branch from this line of work developed Hyperion (2001) and Zoe (2005), solar-powered robots for extended deployment in the arctic tundra and the Atacama Desert. The latest development is a lander/rover combination intended to address the Google X Prize for the first commercially funded venture to put a robot on the moon.<br />",http://www.ri.cmu.edu/person.html?person_id=339,open http://www.ri.cmu.edu/person.html?person_id=339